Phase 1:
***************************************************************
Step 1: Project Initialization
- Create a Directory called 'data':
    mkdir data
- Install the necessary tools:
    pip install pandas scikit-learn mlflow dvc dvc-s3
- Initialize git
    git init
- Create a .gitignore file:
    __pycache__/
    data/*.csv
    dvc_storage/
    mlruns/
    .DS_Store
*************************************************************
Step 2: Download the dataset and split it.
- Link: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
    And extract the zip and store the csv file as full_dataset.csv inside the data folder.
- Create the "Data Manager" Script:
    We need a script that decides how much data we use.
    Create a file manage_data.py.
    In this script, we extract only 10000 rows out of the 50k rows available.
- Generate the version 1 data:
    Run the command: python manage_data.py 1
    Result: You now have a file data/train.csv inside your folder.
************************************************************
Step 3: Setup DVC(Data Version Control)
Now we lock this train.csv file so we can prove to the professor we are tracking versions.
1)Initialize DVC:
    dvc init
2) Set up Local Storage (The "Cloud"):
    mkdir /tmp/dvc_store
    dvc remote add -d mylocal /tmp/dvc_store
3) Track the Data:
    dvc add data/train.csv
    What happened? DVC created data/train.csv.dvc. This is the "fingerprint" file.
4) Save to History:
    git add .
    git commit -m "Initialize DVC with Data Version 1 (10000 rows)"
    dvc push
*****************************************************************
Step 4:
Create the Training Script (with MLflow)

Now we train the model on this real IMDB data.

    Create train.py.
******************************************************************
Step 5:
1) Verify Everything
Run the training: python train.py
2) Launch the MLflow UI to check your work:
    pip install protobuf==4.25.3
    mlflow ui 
Go to localhost:5000
*************************************************************
Summary of Phase 1

    Dataset: Real IMDB data downloaded.

    Incremental Logic: We created manage_data.py which currently outputs Version 1 (small data). Later, we will run it with "2" to output big data.

    DVC: Tracking train.csv.

    MLflow: Logging the experiments.

1. The "Why": Explaining the Tools
Why git init? (We haven't touched GitHub yet!)

You asked: "Why are we doing git init when I am not even connecting to my github account?"

    The Answer: Git is not GitHub.

        Git is a time machine that lives on your laptop. It records changes.

        GitHub is just a website where you upload those records later.

    Why we need it now: DVC requires Git to work. DVC is "Git for big files." It can't function unless there is a Git repository to "piggyback" on. When you run dvc init, DVC checks: "Is this folder a Git repo?" If not, it crashes.

    What happened: When you typed git init, you created a hidden folder .git/ that starts tracking every file change locally.

What is DVC (Data Version Control)?

    The Problem: Git is designed for text code (kilobytes). If you put a 2GB CSV file in Git, it will slow down and eventually crash.

    The DVC Solution: DVC acts like a librarian.

        It takes your heavy train.csv.

        It calculates a unique "fingerprint" (hash) for that file.

        It moves the heavy file into a hidden storage area (your cache).

        It creates a tiny text file named train.csv.dvc that contains just the fingerprint.

    The Trick: You commit the tiny .dvc file to Git. You never commit the heavy CSV. This way, Git tracks which version of the data you used, without holding the data itself.

What is MLflow?

    The Problem: In AI, you run code 50 times with different settings. "Did I use 1000 rows or 5000 rows? Was the accuracy 80% or 82%?" You forget.

    The MLflow Solution: It is an automated lab notebook.

        Tracking: It writes down every parameter (e.g., "Vectorizer=Tfidf") and result (e.g., "Accuracy=0.85").

        Artifacts: It saves the actual "brain" (the .pkl model file) so you can load it later.

2. The Connections: How do they talk to each other?

You asked: "How is dvc connected to my github and mlflow connected to my github?"

Short Answer: They are NOT connected to GitHub directly. They are connected to your folder.
The DVC ↔ GitHub Connection

    It's manual, not magic.

        DVC does not talk to GitHub.

        Git talks to GitHub.

        The Flow:

            DVC creates the tiny train.csv.dvc file.

            Git uploads train.csv.dvc to GitHub.

        The Result: When the professor looks at your GitHub, they see train.csv.dvc. They know exactly what data you used, but the heavy data isn't there.

The MLflow ↔ GitHub Connection

    There is no connection.

        MLflow runs entirely on your laptop (inside the mlruns folder).

        We added mlruns/ to .gitignore. Why? Because we don't want 500 experiment logs cluttering up our GitHub code.

        In Production (Later): In a real company, MLflow would run on a central server. For this project, it lives locally on your machine.
3. Step-by-Step "Under the Hood"

Here is exactly what happened when you ran those commands:
Command -> What actually happened on your hard drive
a) python manage_data.py 1	-> Created Data: A file data/train.csv (10000 rows) was written to your disk.
b) dvc init	-> Setup: DVC created a hidden folder .dvc/ to store its config. It checked "Is Git present?" (Yes).
c) dvc remote add -d mylocal /tmp/dvc_store	-> Storage: You told DVC: "When I say 'push', don't go to AWS S3. Just copy files to my /tmp/dvc_store folder."
d) dvc add data/train.csv	-> 
The Magic:

1. DVC calculated the SHA (fingerprint) of train.csv.

2. It moved the real train.csv into a hidden cache (.dvc/cache).

3. It created a "link" back to data/train.csv so you can still see it.

4. It created data/train.csv.dvc (the claim ticket).

e) git add . ->

Staging: You told Git: "I want to save train.py, .gitignore, and train.csv.dvc."

(Git ignores the heavy train.csv because of .gitignore).
f) python train.py	->

The Experiment:

1. Python read data/train.csv.

2. It trained the model.

3. MLflow created a new folder inside mlruns/ and saved two things: The metrics (Accuracy score) and the Model File (model.pkl).

----------------------------------------------------------------
Phase 2: Building the backend 
1) Step 1: Building the Backend API.
The Goal: We need a "waiter" (the API) that takes an order (a movie review) from the customer, brings it to the chef (the Model), and returns the dish (Positive/Negative sentiment).
We will use FastAPI. It is the modern standard for MLOps because it is fast and automatically creates documentation.

Step 1: Understand the "Link" (The URI)
Before we write code, we need to know where your model lives.
    Go to your terminal where train.py ran.

    Look at the output. You should see a line saying something like: Model saved to MLflow

    Go to your browser (http://localhost:5000).

    Click on your Experiment (IMDB_Sentiment_Analysis).

    Click on the latest Run (the top one).

    Scroll down to the Artifacts section on the right.

    Click on the folder named MLmodel.

    Look for the Full Path or URI. It looks like: runs:/<some-long-id>/model or an absolute path like /home/user/mlruns/....

        Note: For this code, we will use the dynamic "runs" method so you don't have to copy-paste messy paths.

Step 2: Create the Backend Code (app.py)
Create a new file named app.py in your main folder. 

Step 3: Install FastAPI
You need to install the server library (uvicorn) and the API framework (fastapi).
pip install fastapi uvicorn

Step 4: Run the Server
Now, let's turn on the engine.
Run this command:
    uvicorn app:app --reload

    app:app: The first app is the filename (app.py), the second app is the variable name inside the code.

    --reload: This means if you change the code, the server restarts automatically (great for development).

What you should see: You should see green logs saying Uvicorn running on http://127.0.0.1:8000.

Step 5: Test It (The Moment of Truth)

We don't need to build a website to test the backend. FastAPI gives us a free testing website.

    Open your browser and go to: http://127.0.0.1:8000/docs

    You will see a blue page titled "Sentiment Analysis API". This is automatic documentation (Swagger UI).

    Click on the green bar POST /predict.

    Click the Try it out button (top right of the bar).

    In the Request body box, type a test review.

    Click Execute.

Detailed Explanation of What Just Happened

User: "Why did we do all this?"

    Dynamic Loading: The code mlflow.search_runs is smart. It doesn't look for a hardcoded file. It asks MLflow: "What is the newest brain you have?"

        Why this gets marks: This is Automation. If you retrain the model tomorrow, this API will automatically pick up the new one without you changing a single line of code in app.py.

    Pydantic (class Review):

        This protects your API. If someone sends a number 123 instead of text, the API rejects it immediately. This is Reliability.

    The URI (runs:/...):

        This is the link between Model Registry (Phase 1) and Deployment (Phase 2). We are not copying files manually; we are referencing the registry.

That is an excellent observation. You are right—you never wrote a function for /docs, yet it exists.

This is one of the main reasons FastAPI is preferred over older tools like Flask for MLOps.
The "Magic" of /docs

The /docs page is automatically generated by FastAPI. It is not a file you write; it is a dynamic page built in real-time by inspecting your code.

Here is the breakdown of how it works "Under the Hood":
1. It Reads Your Python Types (The Blueprint)

FastAPI looks at this part of your code:
Python

class Review(BaseModel):
    text: str

It sees that text must be a string. It uses this information to build the "Request Body" section in the documentation, telling users exactly what JSON format to send. If you changed str to int, the documentation would instantly update to say "Integer required."
2. It Follows the "OpenAPI" Standard

When you run app = FastAPI(), the library silently creates a hidden file called openapi.json. This file describes every single "door" (endpoint) into your application.

    It sees @app.post("/predict").

    It sees the function description """Receives a review text...""".

    It compiles all of this into a standard format.

3. Swagger UI (The Visuals)

The page you see at /docs is a famous tool called Swagger UI. FastAPI includes it by default. It reads that hidden openapi.json file and turns it into the interactive blue website you are using.
Why this earns you marks

In a professional DevOps/MLOps environment, documentation is mandatory.

    Without FastAPI: You would have to write a separate PDF or Wiki page explaining: "To use my API, send a POST request to /predict..."

    With FastAPI: The code is the documentation. This is called "Self-Documenting Code."

---------------------------------------------------

Phase 3: Building the frontend:
We will use Streamlit. It is incredibly simple—it turns Python scripts into websites without needing HTML or CSS.

Goal: A clean web page where the professor can type a movie review and see the sentiment.

Step 1: Create the Frontend Code (frontend.py)

Create a new file named frontend.py in your main folder.

Step 2: Install Streamlit
    pip install streamlit

Step 3: Run the Frontend

Crucial Note: You need two terminals open now.

    Terminal 1: Must keep running the Backend (uvicorn app:app --reload). Do not close this!

    Terminal 2: Open a new terminal window/tab, navigate to your folder, and run:
    streamlit run frontend.py

What happens:

    A new tab will open in your browser (usually at http://localhost:8501).

    You will see your "Movie Review Sentiment Analyzer" app.

    Type "I loved this movie" and click the button. It should show a green "POSITIVE" box.

----------------------------------------------------------------
Phase 4: Creating Docker files:

1. The High-Level Goal

We took the application that was running loosely on your laptop and packaged it into Docker Containers. This satisfies the requirement for "Containerization: Docker and Docker Compose".

Instead of saying "It works on my machine," we can now say "It works in this box, and you can ship this box anywhere."
2. The Files: A Deep Dive
A. The Backend (app.py) - The "Brain"

    Purpose: This script serves the AI model. It listens for requests and returns predictions.

    The Critical Logic (The Fix): The most important part of this file is the Recursive Model Search.

        Problem: On your laptop, MLflow saved the model path as /home/aayush/.... Inside Docker, that path doesn't exist. Docker puts everything in /app.

        Solution: We wrote code that doesn't care about the absolute path. It looks at the current folder (mlruns) and walks through every sub-folder until it finds model.pkl.

        Code Highlight:
        Python

        for root, dirs, files in os.walk("mlruns"):
            if "model.pkl" in files:
                model_uri = root # Found it!

B. The Frontend (frontend.py) - The "Face"

    Purpose: A Streamlit website for users to interact with the model.

    Key Feature 1 (Networking):

        It uses os.getenv("API_URL", ...) to decide where to send data.

        On your laptop, it defaults to localhost. Inside Docker, we configure it to talk to http://backend:8000.

    Key Feature 2 (Robustness):

        We added an if "error" in data: check. This prevents the website from crashing (Red Screen of Death) if the backend has a hiccup.

C. The Recipes (Dockerfile.backend & Dockerfile.frontend)

These files tell Docker how to build the "Shipping Box."

    FROM python:3.9-slim: "Start with a lightweight Linux computer that already has Python installed."

    WORKDIR /app: "Create a folder called /app and do all work inside it."

    COPY requirements.txt . & RUN pip install ...: "Install all the libraries (FastAPI, Streamlit, MLflow) inside the box."

    COPY . .: "Copy the code from Aayush's laptop into the box."

    EXPOSE 8000 / 8501: "Open a specific port (window) so we can talk to the program inside."

D. The Orchestrator (docker-compose.yml)

This is the manager that runs both boxes at the same time and connects them.

    services:: Defines our two players, backend and frontend.

    depends_on:: Tells the frontend not to start until the backend is running.

    volumes: - ./mlruns:/app/mlruns (THE MOST CRITICAL LINE):

        This creates a "wormhole."

        It connects the mlruns folder on your laptop directly to the /app/mlruns folder inside the container.

        Why this matters: When you train a model on your laptop, the file appears instantly inside the container without you needing to rebuild anything.

    environment: - API_URL=http://backend:8000/predict:

        This tells the Frontend: "Don't look for the backend at 'localhost'. Look for a computer named 'backend' on the internal network."

3. The Commands We Used
1. docker compose up --build

    What it does:

        Reads the Dockerfiles and builds the images (installing Python, libraries, etc.).

        Creates a virtual network (e.g., project_default).

        Starts the containers and streams their logs to your terminal.

    When we used it: Every time we made a small code change.

2. docker compose down

    What it does: Stops the containers and removes them and the network. It cleans up the mess.

    When we used it: Before running the "nuclear" fix to ensure a clean slate.

3. docker compose build --no-cache (The Nuclear Fix)

    Why we needed it: Docker is lazy. If you change a file but Docker thinks "I already built this layer," it ignores your change. We saw this when the logs kept saying "Checked all folders..." even though we deleted that code.

    What it does: It forces Docker to forget everything it knows and rebuild the container from scratch, ensuring the new code is actually used.

User (You) → Browser (localhost:8501)

    Browser → Frontend Container (Streamlit)

    Frontend Container → (via Docker Network) → Backend Container (FastAPI)

    Backend Container → Reads model.pkl from Volume (Your Laptop's Storage)

    Backend → Returns Prediction to Frontend.

You have now completed the Local Development & Containerization phases. The application is portable and ready for the cloud (Kubernetes).
This is one of the most important concepts in Docker networking, and it confuses almost everyone when they first start.

Here is the simple explanation of why we use 0.0.0.0 instead of localhost or 127.0.0.1.
The Short Answer

    127.0.0.1 (Localhost): "I will only talk to processes running inside this exact same container. If you are outside the container (like on your laptop), I will ignore you."

    0.0.0.0: "I will listen to any traffic coming from anywhere. Whether it's from inside the container, from another container, or from your laptop—come on in!"

-----------------------------------------------

Phase 5: Kubernetes
We are now going to stop using Docker Compose (which is for one computer) and switch to Kubernetes (which manages entire clusters).

The Goal:

    Create a Deployment for the Backend (to ensure it stays alive).

    Create a Service for the Backend (so the Frontend can find it).

    Create a Deployment & Service for the Frontend (so you can access it).

Note: For the backend to work in K8s, it needs to access your model. In a real production cluster, we would use a Persistent Volume (PV). Since you are running Minikube or local K8s, we will use a HostPath Volume (just like we did in Docker Compose).
Step 1: Create the Backend Manifest (k8s-backend.yaml)

Create a file named k8s-backend.yaml -> This has the code to create the backend deployment and service.

Step 2: Create the Frontend Manifest (k8s-frontend.yaml)

Create a file named k8s-frontend.yaml -> This has the code to create the frontend deployment and service.

Step 3: Point Minikube to Local Docker (CRITICAL STEP)

By default, Minikube doesn't see the Docker images you built on your laptop. It has its own internal Docker. We need to point your terminal to Minikube's Docker daemon so we can build the images inside Minikube.

a) Switch context -> eval $(minikube docker-env)
b) Rebuild images (Inside Minikube): Now that we switched environments, we need to build the images again so they exist inside the cluster.
docker build -f Dockerfile.backend -t project-backend:latest .
docker build -f Dockerfile.frontend -t project-frontend:latest .

Step 4: Deploy!

    Apply Backend: kubectl apply -f k8s-backend.yaml
    Apply Frontend: kubectl apply -f k8s-frontend.yaml

Step 5: Access the App

Since you are using NodePort, Minikube gives you a special command to open the URL directly.
minikube service frontend-service

Minikube runs inside a virtual machine (or a container). It is like a separate computer running inside your laptop. It cannot see your laptop's hard drive automatically.
IMPORTANT: Step 1: Mount the Folder (The Bridge)

Crucial: You must do this in a new terminal window, and you must keep this window open as long as your app is running.

    Open a new terminal.

    Run this exact command (I added quotes because your path has spaces in "Semester 7"):
    minikube mount "/home/aayush-bhargav/Documents/Semester 7/Software Production Engineering/Project/mlruns":/mnt/mlruns
    Left Side: Your laptop's real folder.

    Right Side: The simplified path inside Minikube (/mnt/mlruns).

Wait until you see a message like Mounting host path ... or User defined volume location.... Do not close this terminal

Step 2: Update the YAML

Now we need to tell Kubernetes to look at the bridge (/mnt/mlruns), not the original path.

    Open k8s-backend.yaml.

    Scroll down to the volumes section.

    Change the path to /mnt/mlruns.

----------------------------------------------------------

Phase 6: CI/CD Automation with Jenkins.

The Goal: We want to automate the workflow so that when you push code to GitHub, Jenkins wakes up, tests it, builds the Docker images, uploads them to the cloud (Docker Hub), and updates your Kubernetes cluster.

Step 0: Create two repositories on your Docker Hub:
    Name one: mlops-backend
    Name the second: mlops-frontend
Step 1: Install "Docker Pipeline" plugin
Step 2: Add Docker Hub Credentials to Jenkins (Already done in mini Project)
Step 3: Create the Jenkinsfile
The Jenkinsfile is a text file that tells Jenkins what to do. It lives in your project folder.
    Create a file named Jenkinsfile (no extension) in your main folder.
Step 4: Prepare Your Kubernetes YAMLs for the Cloud

Since we are now pushing to Docker Hub, your Kubernetes files need to pull from there, not locally.

    Open k8s-backend.yaml:

        Change image: project-backend:latest

        To: image: YOUR_DOCKERHUB_USER/mlops-backend:latest

        Change imagePullPolicy: Never

        To: imagePullPolicy: Always

    Open k8s-frontend.yaml:

        Change image: project-frontend:latest

        To: image: YOUR_DOCKERHUB_USER/mlops-frontend:latest

        Change imagePullPolicy: Never

        To: imagePullPolicy: Always
Step 5:
Step 6: Push to GitHub

Jenkins needs to fetch the Jenkinsfile from GitHub.

    Go to GitHub.com and create a New Repository named sentiment-mlops.

    Run these commands in your terminal to upload your code:
    git remote add origin https://github.com/YOUR_GITHUB_USER/sentiment-mlops.git
    git add .
    git commit -m "Added Jenkinsfile and updated K8s manifests"
    git branch -M main
    git push -u origin main
Step 7: Create the Jenkins Job

    Go to Jenkins Dashboard (http://localhost:8080).

    Click New Item.

    Name: mlops-pipeline.

    Select Pipeline and click OK.

    Scroll down to Pipeline section.

    Definition: Pipeline script from SCM.

    SCM: Git.

    Repository URL: Paste your GitHub URL (e.g., https://github.com/User/repo.git).

    Branch Specifier: */main.

    Trigger: Use Github webhook-> Set up the webhook as in slides.

Important stuff: 
-> Verify Jenkins Permission (Crucial)
Since you are not using the specific Docker container command I gave earlier, you likely have Jenkins installed directly on your machine. Problem: The jenkins user often doesn't have permission to run docker commands by default. Fix: Run this command in your terminal to give Jenkins permission:
sudo usermod -aG docker jenkins
sudo systemctl restart jenkins

-> Step 2: Expose Jenkins to the Internet (For Webhooks)

You want to use GitHub Webhooks. The Problem: GitHub is on the internet; your Jenkins is on localhost. GitHub cannot "see" your laptop to send the trigger. The Solution: Use Ngrok to create a tunnel.
ngrok http 8080

What exactly is being tested?

Unlike your previous Java project where mvn test ran hundreds of tiny unit tests, here we are running an Integration/Smoke Test by executing your actual AI training script (train.py).

Running python3 train.py verifies five critical things at once. If this script finishes without crashing, the test Passes. If it crashes, the pipeline Fails.

    Syntax Check: It proves your Python code (app.py, train.py) doesn't have any typos or syntax errors that would stop it from running.

    Dependency Check: It proves all your libraries (pandas, sklearn, mlflow) are installed and working correctly in the Jenkins environment.

    Data Integrity (DVC): It proves your dataset (data/train.csv) is readable and formatted correctly. If DVC failed to pull the data, this script would crash immediately.

    MLflow Connection: It proves the code can successfully talk to MLflow to log metrics and save the model.

    Model Viability: It proves the machine learning algorithm (Logistic Regression) can actually learn from your data without mathematical errors

What running python train.py actually proves:

When Jenkins runs this command, it isn't just "training" the model; it is secretly verifying 5 critical things that could break your project:

    The Environment is Correct:

        If you forgot to install pandas or scikit-learn in the Jenkins environment, train.py will crash immediately with ModuleNotFoundError.

        Result: The test fails, preventing you from building a broken Docker image.

    The Data is Accessible:

        The script tries to read data/train.csv. If DVC failed to pull the data, or the file is missing/corrupt, the script crashes.

        Result: It proves your Data Pipeline is working.

    The Syntax is Valid:

        If you made a typo in your Python code (e.g., missed a closing parenthesis )), the script won't run.

        Result: It proves your code is valid Python.

    MLflow is Reachable:

        The script tries to log metrics to MLflow. If the MLflow server is down or the directory permissions are wrong, it crashes.

        Result: It proves your Logging infrastructure is up.

    Mathematical Viability:

        If your data has "NaN" (empty values) or text where numbers should be, the model.fit() command will error out.

        Result: It proves your data cleaning logic was successful.
-> Very IMPORTANT: 
One Pre-requisite Command

Sometimes the "virtual environment" tool is missing on Linux by default. Run this command in your terminal (not Jenkins) to make sure it is installed:
sudo apt-get install python3-venv

Also Important:
Give Jenkins the Map

We need to give the jenkins user a copy of your Kubernetes configuration file so it knows where to find the cluster.
Step 1: Copy the Kubeconfig

Run these 3 commands in your local terminal to copy your config to a place Jenkins can read:
# 1. Copy your personal config to the Jenkins folder
sudo cp ~/.kube/config /var/lib/jenkins/kubeconfig

# 2. Give ownership to the jenkins user
sudo chown jenkins:jenkins /var/lib/jenkins/kubeconfig

# 3. Make it readable
sudo chmod 600 /var/lib/jenkins/kubeconfig

# 1. Create a 'flattened' config that embeds the keys directly into the file
# --flatten: Replaces file paths with the actual certificate data
# --minify: Only keeps the current cluster info (removes clutter)
kubectl config view --flatten --minify > temp_kubeconfig

# 2. Move this new self-contained file to the Jenkins folder
# (We overwrite the old broken one)
sudo mv temp_kubeconfig /var/lib/jenkins/kubeconfig

# 3. Give ownership to Jenkins again
sudo chown jenkins:jenkins /var/lib/jenkins/kubeconfig
sudo chmod 600 /var/lib/jenkins/kubeconfig


This is the final piece of the puzzle that closes the "DevOps Loop."

Here is the breakdown of exactly what is happening in that stage and what happens to your application.
1. What is this stage doing?

It is performing a Rolling Update.

    The Command: kubectl rollout restart deployment/...

        This tells Kubernetes: "Hey, I know the YAML configuration hasn't changed, but I want you to kill the current pods and start fresh ones anyway."

    The Flag: --kubeconfig=/var/lib/jenkins/kubeconfig

        This tells Jenkins: "Use the specific 'Map' (config file) we created earlier to find the cluster. Do not try to guess where it is."

Why is this necessary? In the previous stage (Push to Docker Hub), you uploaded a new version of your image (e.g., you changed the code). However, Kubernetes is lazy. If you don't tell it to restart, it will keep running the old image forever because the Deployment YAML itself didn't change. By forcing a restart, Kubernetes sees imagePullPolicy: Always, checks Docker Hub, sees the newer image hash, and downloads the fresh code.
2. Are the pods going to restart?

Yes, but intelligently.

Kubernetes does not just kill everything at once (which would crash your website). It performs a "Zero-Downtime Deployment" (even with 1 replica, it tries to be fast, though with multiple replicas it is seamless).

The Sequence of Events:

    Kubernetes: "Okay, time to restart."

    Action: It creates a New Pod (running the new code) alongside the Old Pod.

    Wait: It waits for the New Pod to say "I am Ready" (Running state).

    Switch: It switches the traffic from Old → New.

    Kill: It terminates the Old Pod.

So, your kubectl get pods will temporarily show two backend pods (one Terminating, one Running) before settling back to one.
3. How do I access the website?

Since you are using Minikube with a NodePort, the address remains exactly the same as before. The "Door" (Port 30001) doesn't move; only the "Room" behind it changes.

Run this command in your local terminal:
minikube service frontend-service

Or, if you manually set the port to 30001, you can simply go to: http://<MINIKUBE-IP>:30001 (Run minikube ip to see the IP, usually 192.168.49.2).
Summary of your Full Pipeline Execution

    You Push Code → GitHub.

    GitHub → Pokes Jenkins (Webhook).

    Jenkins →

        Creates a dummy dataset.

        Runs train.py to prove code works.

        Builds Docker Images.

        Pushes to Docker Hub.

        Tells Minikube to Restart.

    Minikube → Pulls new image → Website updates automatically.

You have now achieved full CI/CD automation.

Very Important:
Before the demo: Kill the terminal window, open a fresh one, and run the mount command newly just to be safe.
minikube mount "/home/aayush-bhargav/Documents/Semester 7/Software Production Engineering/Project/mlruns":/mnt/mlruns


The Final Workflow (How to Demo It)

Now, here is how you prove "Automation" to the professor:

    Show the current website: It says "Model Version: X".

    Make a Change:

        Open train.py.

        Change a print statement or small logic (e.g., change max_iter=100 to max_iter=200).

        Narrative: "I am updating the training parameters."

    Push to Git:

        git add .

        git commit -m "Retraining model with new params"

        git push

    Hands Off:

        Show Jenkins starting automatically.

        Jenkins Trains a new model (creates new mlruns).

        Jenkins Bakes it into the Docker image.

        Jenkins Pushes to Docker Hub.

        Jenkins Restarts Kubernetes.

    Result:

        Refresh the website.

        The "Model Version" will change automatically.

        NO manual commands.

--------------------------------------------------

Also, very important

tep 1: Fix DVC Permissions (The Bridge)

Your DVC storage lives at /tmp/dvc_store (or wherever you set it). Currently, only you (aayush) can read it. We need to let Jenkins read it too.

Run this command in your local terminal:
Bash

# 1. Give everyone Read/Write access to the DVC storage folder
# This allows Jenkins to 'pull' the data you 'pushed'
sudo chmod -R 777 /tmp/dvc_store

(Note: If you stored DVC somewhere else, replace /tmp/dvc_store with that path).

Step 3: The New Workflow (How you trigger it)

Now, whenever you want to update the model, follow this strict sequence. This is the workflow you show the professor.

    Modify Data (Laptop):

        Open data/train.csv.

        Add 50 new rows (or copy-paste a dataset).

        Save.

    Sync Data (DVC):

        dvc add data/train.csv

        dvc push

        Result: Your new data is now sitting in /tmp/dvc_store.

    Sync Code (Git):

        git add data/train.csv.dvc (Notice: you add the .dvc file, not the csv)

        git commit -m "Updated dataset with new rows"

        git push origin main

    Automation (Jenkins):

        Jenkins wakes up.

        It runs dvc pull.

        It grabs your new data from /tmp/dvc_store.

        It trains a new model.

        It deploys it to Kubernetes.

Apply these changes. This gives you the exact behavior you wanted: Your laptop drives the data, Jenkins drives the deployment.


This is the final piece of the "MLOps Puzzle." You are asking how to manage the lifecycle of your project (History, Versions, and Tracking).

Here is exactly how it works with the system we built.
Question 1: "If I make a change to train.csv or train.py and do git push, does it work?"

The answer is Yes for code, but No for data (unless you use DVC commands).
Scenario A: Changing Code (train.py)

This is standard Git.

    Modify train.py.

    git add train.py

    git commit -m "Changed model parameters"

    git push

    Result: Jenkins triggers, pulls the code, runs the pipeline, and updates the website.

Scenario B: Changing Data (train.csv)

You cannot just git push the CSV because we put it in .gitignore. You must use DVC.

    Modify data/train.csv (add rows).

    DVC Step: dvc add data/train.csv

        This updates the train.csv.dvc file (the "pointer").

    DVC Sync: dvc push

        This sends the actual heavy data to your storage locker (/tmp/dvc_store).

    Git Step: git add data/train.csv.dvc

        You commit the pointer, not the data.

    git commit -m "Updated dataset with new rows"

    git push

    Result: Jenkins sees the .dvc file changed. It runs dvc pull to get the new heavy data from /tmp/dvc_store, trains a new model, and updates the website.

Question 2: "How do I version control the data and keep track of models?"

In MLOps, we don't just "remember" things. We link them together in a chain.

The Golden Chain of MLOps: Git Commit SHA → DVC Data Hash → MLflow Run ID → Docker Image Tag.

Here is how you track everything in your project:
1. Tracking Data Versions (Time Travel)

DVC lets you switch between data versions just like Git branches.

    Command: git checkout HEAD~1 (Go back to yesterday's code).

    Command: dvc checkout

    Effect: DVC looks at the old .dvc file and instantly restores data/train.csv to exactly how it looked yesterday.

    Why this gets marks: You can prove to the professor: "I can restore the exact dataset used to train the model 3 weeks ago."

2. Tracking Model Runs (MLflow)

Currently, our Jenkinsfile does rm -rf mlruns to avoid permission errors, which deletes history. To "Keep Track" like a pro, we need to stop deleting it and make it Persistent.

The Fix: Use a Jenkins Volume for MLflow History We will tell Jenkins: "Don't save logs in the temporary workspace. Save them in a permanent folder so I can see the history of ALL my models."

Step A: Create the Permanent Folder Run this in your terminal:
sudo mkdir -p /var/lib/jenkins/mlflow_history
sudo chown -R jenkins:jenkins /var/lib/jenkins/mlflow_history
sudo chmod -R 777 /var/lib/jenkins/mlflow_history

Step B: Update Jenkinsfile Change the Train Model (CI) stage to use this folder.
Groovy

        stage('Train Model (CI)') {
            steps {
                echo 'Training with Persistent History...'
                sh '''
                python3 -m venv venv
                . venv/bin/activate
                pip install --upgrade pip
                pip install -r requirements.txt
                
                # 1. SETUP DVC
                dvc remote add -d -f mylocal /tmp/dvc_store
                dvc pull
                
                # 2. LINK TO PERMANENT HISTORY (The Trick)
                # Instead of deleting mlruns, we link it to the permanent folder
                # This way, Run 1, Run 2, Run 3... are all saved forever.
                ln -s /var/lib/jenkins/mlflow_history mlruns
                
                # 3. TRAIN
                # MLflow will now append the new run to the history
                python3 train.py
                
                # 4. UNLINK (Cleanup for Docker build)
                # We need to copy the *content* into the docker image, not the link
                rm mlruns
                cp -r /var/lib/jenkins/mlflow_history mlruns
                '''
            }
        }

Summary for Your Demo

When the professor asks "How do you track versions?", you show this slide:

    Code Version: "GitHub tracks the app.py changes."

    Data Version: "DVC tracks the train.csv changes. I can use dvc checkout to revert data."

    Model Version: "MLflow tracks every training run. My Jenkins pipeline saves these logs to a persistent volume, so we have a history of accuracy over time."

    Deployment Version: "Docker Hub has tagged images (:build-1, :build-2). If Production breaks, I can instantly roll back Kubernetes to image:build-1."

Recommendation: For the immediate next step, you have two choices:

    Stick to what works: Keep rm -rf mlruns. It's safer and less prone to permission errors during the demo. You lose history, but the pipeline is bulletproof.

    Enable History: Do the mkdir and ln -s steps above. It gives you the "History" feature but adds complexity.

I suggest sticking to what works for now. You already have the DVC + Git + Docker linkage, which is enough for the "Versioning" marks.


TODO:
Phase 7: Scalability (Horizontal Pod Autoscaling)

Goal: Prove the application can handle a traffic spike.

    Enable Metrics: We turn on the "speedometer" in Minikube so it can measure CPU usage.

    Set Limits: We artificially limit the Backend Pod to use very little CPU (e.g., "10% of a core").

    Create Autoscaler: We tell Kubernetes: "If usage hits 50%, multiply the pods up to 5."

    Stress Test: We run a "load generator" loop that floods the API with requests, forcing Kubernetes to spin up 4 extra pods in real-time.

Phase 8: Configuration Management (Ansible)

Goal: Prove you can automate server setup, not just application deployment.

    The Scenario: Imagine you bought a brand new, empty server (VM). You can't deploy your K8s app yet because it doesn't even have Docker or Kubernetes installed.

    The Playbook: We write an Ansible Playbook (a script) that:

        Updates the Linux system.

        Installs Docker.

        Installs Kubernetes tools (kubectl).

    The Demo: You run one command (ansible-playbook setup.yml), and it turns a "dumb" server into a "ready-to-deploy" server. This satisfies the "Modular Design / Roles" requirement.

Phase 9: Security (HashiCorp Vault)

Goal: Prove you aren't hardcoding passwords in your code.

    The Problem: Right now, if we had a database password, we might be tempted to put it in k8s-backend.yaml. That is a security risk.

    The Vault: We start a HashiCorp Vault service.

    The Secret: We store a fake API Key (e.g., SECRET_KEY=12345) inside Vault.

    The Integration: We update the Backend Pod to "fetch" this secret from Vault when it starts, instead of reading it from a file.

Which one would you like to execute next? (I recommend Phase 7 (HPA) first as it connects directly to the Kubernetes work we just finished.)

-----------------------------------

Project Report: ELK Stack Integration for MLOps Observability
1. Objective

The goal was to move beyond simple console printing and implement a centralized logging system. This ensures that in a distributed system (Kubernetes), all application logs are aggregated, searchable, and visualizable in real-time. This satisfies the project requirement for "Monitoring and Observability."
2. Architecture Overview

Due to resource constraints on the local Minikube environment, we implemented a "Slim" ELK Architecture:

    Elasticsearch (The Database): Stores and indexes log data.

    Kibana (The Dashboard): Provides a UI to search and visualize logs.

    Direct Ingestion (No Logstash): Instead of running a heavy Logstash container, the Python backend formats logs as JSON and ships them directly to Elasticsearch via HTTP.

3. Implementation Details
A. Infrastructure Setup (Kubernetes)

We created a new manifest, k8s-logging.yaml, to deploy the logging infrastructure.

    Elasticsearch: Deployed as a single-node cluster with memory limits (512Mi - 1Gi) to prevent system crashes. Exposed internally on port 9200.

    Kibana: Deployed and connected to Elasticsearch. Exposed via NodePort 30002, making the dashboard accessible at http://<NodeIP>:30002.

B. Application Integration (Python Backend)

We modified app.py to act as a log shipper.

    Custom Handler: We encountered a compatibility issue with the cmreslogging library on Python 3.12. We resolved this by implementing a custom class, SimpleElasticsearchHandler. This handler uses the standard requests library to send asynchronous POST requests to the Elasticsearch service.

    Structured Logging: We replaced standard print statements with log.info() and log.error().

    Contextual Data: The logs are not just text; they now include structured timestamps (datetime.timezone.utc) and event details (e.g., "AI Prediction: Positive", "Database Success").

Key Code Snippet (The Custom Handler):
Python

class SimpleElasticsearchHandler(logging.Handler):
    def emit(self, record):
        payload = {
            "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "level": record.levelname,
            "message": self.format(record),
            # ... additional metadata
        }
        requests.post(self.url, headers=self.headers, json=payload, timeout=1)

C. Pipeline Automation (Jenkins)

The Jenkinsfile was updated to automate the deployment of the logging stack.

    Stage Update: The "Update Kubernetes" stage was modified to apply the logging infrastructure before the application deployment:
    Groovy

    sh "kubectl --kubeconfig=... apply -f k8s-logging.yaml"

    This ensures the logging database is ready before the backend starts trying to send logs.

4. Configuration & Validation

    Index Management: We verified that logs were physically reaching the database by checking Stack Management > Index Management, which showed the rotten_potatoes_logs index containing documents.

    Index Patterns: We configured Kibana to recognize the incoming data by creating an index pattern rotten_potatoes_logs* and linking it to the @timestamp field.

5. Data Flow Summary

    Trigger: User submits a movie review on the Frontend.

    Processing: Backend (app.py) receives the request and runs the ML model.

    Logging: The app generates a JSON log entry: {"level": "INFO", "message": "Prediction: Positive"}.

    Transport: The SimpleElasticsearchHandler sends this JSON via HTTP to http://elasticsearch:9200.

    Visualization: The Admin views the log in real-time on the Kibana Dashboard (Port 30002).

6. Outcome

The system now possesses a robust observability layer. We can trace a request from the moment it hits the API, through the ML prediction process, to its final storage in PostgreSQL, entirely through the Kibana interface without needing to SSH into containers

Access Kibana:

    Run kubectl port-forward svc/kibana 5601:5601 in a new terminal.

    Open http://localhost:5601.

    Go to Management > Stack Management > Index Patterns.

    Create a pattern for rotten_potatoes_logs*.

    Go to Discover. You will see your real-time application logs appearing on the dashboard!


TODO
To make this work in Ansible, your teammate needs to use the Kibana API. Kibana allows you to create index patterns programmatically by sending a specific "Internet Signal" (HTTP Request) to it.

Here is exactly what your teammate needs to add to their Ansible playbook (playbook-2.yml) to automate the ELK setup fully.
The Strategy: "Talk to the Robot, not the Screen"

Instead of a human opening Chrome, Ansible will use the uri module (which is like curl) to tell Kibana: "Hey, create an index pattern for rotten_potatoes_logs* right now."
The Code to Add (Copy-Paste this for your teammate)

They should add these tasks to playbook-2.yml right after the "Update Kubernetes Deployment" section.
YAML

    # ... (Previous tasks apply k8s-backend, k8s-database, etc.) ...

    - name: 5. DEPLOY LOGGING INFRASTRUCTURE
      ansible.builtin.shell: |
        kubectl --kubeconfig={{ kubeconfig_path }} apply -f k8s-logging.yaml
        # Wait for Kibana to actually start (it takes time!)
        kubectl --kubeconfig={{ kubeconfig_path }} rollout status deployment/kibana --timeout=300s
      args:
        chdir: /home/{{ ansible_user }}/project_workspace

    - name: 6. CONFIGURE KIBANA (The Magic Step)
      # This task hits the Kibana API to create the index pattern automatically.
      # It replaces the manual "Stack Management > Create Index Pattern" clicks.
      ansible.builtin.uri:
        url: "http://localhost:30002/api/saved_objects/index-pattern/rotten_potatoes_pattern"
        method: POST
        headers:
          kbn-xsrf: "true" # Required by Kibana to prove this isn't a hacker attack
          Content-Type: "application/json"
        body_format: json
        body:
          attributes:
            title: "rotten_potatoes_logs*"
            timeFieldName: "timestamp"
        status_code: [200, 409] # 200 = Created, 409 = Already exists (don't fail if run twice)
      register: kibana_response
      retries: 10      # If Kibana isn't ready yet...
      delay: 10        # ...wait 10 seconds and try again (up to 100s total)
      until: kibana_response.status == 200 or kibana_response.status == 409

Breakdown of what this does:

    Deploy & Wait:

        It applies the k8s-logging.yaml file.

        Crucial: It runs rollout status. This pauses the Ansible script until Kibana is 100% running. If we tried to configure it while it was booting, it would fail.

    The API Call (ansible.builtin.uri):

        URL: It talks to localhost:30002 (where Kibana lives).

        Endpoint: /api/saved_objects/index-pattern/... is the specific door for creating patterns.

        kbn-xsrf: true: This is a special header Kibana requires for security. Without it, the API ignores you.

        Body: It sends the exact configuration you typed manually (title: "rotten_potatoes_logs*", timeFieldName: "timestamp").

    Robustness (retries):

        Even after the pod starts, Kibana takes about 1 minute to "warm up" its internal database.

        The retries: 10 loop ensures that if Ansible tries too early and gets a "Connection Refused," it will just wait and try again until it works.

The Result

When this playbook runs:

    Kubernetes starts the ELK stack.

    Ansible waits for it to turn green.

    Ansible configures the Index Pattern.

    You open the browser, and the logs are already there. No manual setup required.